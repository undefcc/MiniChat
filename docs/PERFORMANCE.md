# 性能优化实战日志

这里记录项目演进过程中遇到的真实性能瓶颈与解决思路。

## 01. 告别卡顿：IoT 监控大屏的"多动症"治疗方案 (2026-02-06)

### 🚨 案发现场
随着接入的 Edge 站点从原本演示用的 3 个增加到压力测试的 50+ 个，我们的 "IoT 总控大屏" 开始出现明显的不适。
只要开启 WebSocket 连接，页面帧率肉眼可见地下降。即使操作者什么都不干，浏览器进程 CPU 占用率也飙升不下。用户反馈："我就想点一下某个站点的详情，结果点击甚至有 200ms 的延迟。"

### 🕵️‍♂️ 侦探分析
打开 React DevTools 的 "Highlight updates" 就像看烟花一样——整个屏幕都在闪烁。
罪魁祸首在于数据流向：
1.  **高频轰炸**：50 个站点，每个站点每秒上报一次状态，意味着每秒有 50 个 WebSocket 事件。
2.  **牵一发而动全身**：状态存储在 `useStationMonitor` Hook 中，它挂载在 `IoTPage` 父组件上。
3.  **渲染雪崩**：每收到一个 `{ stationId: "A", battery: 99 }` 的包，Hook 触发更新 -> `IoTPage` 重绘 -> **列表里的 50 个 `StationItem` 全部被迫重绘**。
这意味着每秒钟我们试图执行 `50 * 50 = 2500` 次组件渲染逻辑。React 即使再快，也扛不住这种无意义的消耗。

### 🛠️ 实施手术：外科式原子化更新
我们需要切断 "父组件状态变更 -> 子组件全量刷新" 的链路。决定引入 **Zustand** 进行状态管理改造。

1.  **状态下沉与逃逸**：
    我们将 `stationStatusMap`（包含了所有设备实时状态的巨型对象）从 React 组件树中"偷"出来，放到了全局 Store 中。
    现在 `useStationMonitor` 收到 Socket 数据时，不再调用 `setState`，而是直接更新 Store。这意味着 **React 甚至不知道数据变了**，父组件完全保持静止。

2.  **组件的精准打击**：
    重构列表组件，将其封装为 `StationListItem`。利用 Zustand 的 Selector 机制，让每个组件只"盯着"属于自己的那一份数据：
    ```typescript
    // 只有当 stationStatusMap[id] 变化时，这个组件才渲染
    const status = useStationStore(s => s.stationStatusMap[id])
    ```

### ✅ 治疗效果
改造完成后，我们在 Mock 脚本中将数据频率调高了 5 倍（250 TPS）。
*   **Before**: 页面几乎卡死，FPS 跌至个位数。
*   **After**: 列表静如止水，只有数据真正变化的那个小图标会在瞬间刷新。CPU 占用率从 80% 降至 5% 以下。

这不仅是一次代码重构，更是将渲染复杂度从 **O(N)** 降维打击到了 **O(1)**。

## 02. 治本之策：信令服务的流量大坝 (2026-02-06)

### 🌊 洪水警报
随着前端性能问题的解决，我们发现了一个新的隐患。虽然浏览器不再卡顿，但网络面板里的 WebSocket 帧数却像瀑布一样刷屏。
如果有 100 个边缘站点，每个站点以 5Hz 的频率上报数据（这在实时监控中很常见），那么信令服务器每秒就要处理 500 个入站请求，并向所有在线的浏览器客户端广播 500 次。
如果我们有 10 个管理员同时在线，后端的出站吞吐量就是 `500 * 10 = 5000` 包/秒。这对于 Node.js 单线程模型来说，序列化 JSON 和网络 IO 的开销是巨大的浪费。

### 🧩 策略转型
我们意识到，对于人类观察者来说，"实时"并不意味着毫秒级的同步。人眼的反应速度和屏幕的刷新率决定了，每秒更新 1-2 次状态已经足够"实时"了。
于是，我们在后端（Signaling Gateway）实施了 **"聚合广播 (Batch Broadcast)"** 策略。

### 🏗️ 筑坝蓄水
1.  **设立缓冲区**：
    后端不再是"无情的转发机器"。收到边缘端上报的 `station-status-update` 后，不再直接 Emit，而是存入一个 `Map` 缓冲区。
    ```typescript
    // Buffer: { "station-1": { temp: 45 }, "station-2": { temp: 60 } }
    this.statusBuffer.set(stationId, payload);
    ```
    妙处在于：如果同一秒内 Station-1 上报了 5 次，我们只保留最后一次（最新的那个）。这天然实现了一种服务器端的 **Debounce（防抖）**。

2.  **定期泄洪**：
    启动一个 1000ms 的 Ticker（定时器）。每秒钟醒来一次，将 Buffer 里所有积累的变更打包成一个大数组，通过 `batch-station-status-update` 事件一次性发送。

### 📉 降维打击
这一改动带来的收益是惊人的：
*   **网络包数量降低 99%**：无论边缘端上报频率多高，浏览器端每秒只收到 **1 个** 数据包。
*   **带宽节省**：WebSocket 帧头（Frame Header）的开销大幅减少。
*   **前端更从容**：前端的状态管理 Store 不再需要每秒处理数百次微小的 Commit，而是每秒只进行一次批量 Commit，极大地降低了主线程压力。

现在，整套系统正如同一条精心设计的水利工程：上游（Edge）可以狂风暴雨，中游（Server）负责大坝蓄水，下游（Client）只看到涓涓细流，平稳而有序。

---
*待续：后续将记录关于 WebRTC 点对点连接建立速度优化、视频流内存泄漏排查等实战案例。*
